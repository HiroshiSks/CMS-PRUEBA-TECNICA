{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c577850b",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b18cd3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfc3f6a",
   "metadata": {},
   "source": [
    "# lectura"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be87f9df",
   "metadata": {},
   "source": [
    "# funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7eb0897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "root_excels = \"C:\\\\Users\\\\hsakurada\\\\Desktop\\\\ESTUDIO\\\\wea\\\\CMS-PRUEBA-TECNICA\\\\descargas\"\n",
    "\n",
    "MESES = {\n",
    "    \"enero\": 1, \"febrero\": 2, \"marzo\": 3, \"abril\": 4,\n",
    "    \"mayo\": 5, \"junio\": 6, \"julio\": 7, \"agosto\": 8,\n",
    "    \"septiembre\": 9, \"octubre\": 10, \"noviembre\": 11, \"diciembre\": 12\n",
    "}\n",
    "aÃ±os = ['2025','2024','2023','2022','2021','2020','2019','2018','2017','2016','2015','2014','2013','2012','2011','2010']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bef3e92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extraer_mes_nomb_excel(nombre_archivo):\n",
    "    # quitar extensiÃ³n\n",
    "    nombre = os.path.splitext(nombre_archivo)[0].lower()\n",
    "\n",
    "    # separar por espacios, guiones, underscores, etc.\n",
    "    partes = re.split(r\"[ _\\-]+\", nombre)\n",
    "\n",
    "    # buscar coincidencia exacta\n",
    "    for parte in partes:\n",
    "        if parte in MESES:\n",
    "            return parte  # retorna el nombre del mes encontrado\n",
    "\n",
    "    # bÃºsqueda mÃ¡s flexible: \" - diciembre 2012\"\n",
    "    for mes in MESES:\n",
    "        if mes in nombre:\n",
    "            return mes\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def extraer_aÃ±o(nombre_archivo):\n",
    "    \n",
    "    # quitar extensiÃ³n\n",
    "    nombre = os.path.splitext(nombre_archivo)[0].lower()\n",
    "\n",
    "    # separar por espacios, guiones, underscores, etc.\n",
    "    partes = re.split(r\"[ _\\-]+\", nombre)\n",
    "\n",
    "    # buscar coincidencia exacta\n",
    "    for parte in partes:\n",
    "        if parte in aÃ±os:\n",
    "            return parte  # retorna el aÃ±o encontrado\n",
    "\n",
    "    # bÃºsqueda mÃ¡s flexible: \" - diciembre 2012\"\n",
    "    for aÃ±o in aÃ±os:\n",
    "        if aÃ±o in nombre:\n",
    "            return aÃ±o\n",
    "\n",
    "    return None\n",
    "\n",
    "import os\n",
    "\n",
    "def listar_todos_los_root_excels(root_excels):\n",
    "    excels = []\n",
    "    for root, dirs, files in os.walk(root_excels):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.xlsx', '.xls')):\n",
    "                excels.append(os.path.join(root, file))\n",
    "    return excels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4656f5cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\hsakurada\\\\Desktop\\\\ESTUDIO\\\\wea\\\\CMS-PRUEBA-TECNICA\\\\descargas\\\\2020\\\\Base de Datos de Transporte PÃºblico - Abril 2021.xlsx',\n",
       " 'C:\\\\Users\\\\hsakurada\\\\Desktop\\\\ESTUDIO\\\\wea\\\\CMS-PRUEBA-TECNICA\\\\descargas\\\\2020\\\\Base de Datos de Transporte PÃºblico - Agosto 2021.xlsx',\n",
       " 'C:\\\\Users\\\\hsakurada\\\\Desktop\\\\ESTUDIO\\\\wea\\\\CMS-PRUEBA-TECNICA\\\\descargas\\\\2020\\\\Base de Datos de Transporte PÃºblico - Diciembre 2021.xlsx',\n",
       " 'C:\\\\Users\\\\hsakurada\\\\Desktop\\\\ESTUDIO\\\\wea\\\\CMS-PRUEBA-TECNICA\\\\descargas\\\\2020\\\\Base de Datos de Transporte PÃºblico - Enero 2021.xlsx']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rutas = listar_todos_los_root_excels(root_excels)\n",
    "rutas[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf8695a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# ============================================================\n",
    "# PRECOMPILAR REGEX (20x mÃ¡s rÃ¡pido si se usa miles de veces)\n",
    "# ============================================================\n",
    "_re_non_alnum = re.compile(r\"[^0-9a-z]+\")\n",
    "_re_multi_unders = re.compile(r\"_+\")\n",
    "\n",
    "def normalize_fast(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normaliza texto MUY rÃ¡pido: sin acentos, minÃºsculas, underscores limpios.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    text = \"\".join(c for c in text if not unicodedata.combining(c))\n",
    "    text = text.lower()\n",
    "\n",
    "    text = _re_non_alnum.sub(\"_\", text)\n",
    "    text = _re_multi_unders.sub(\"_\", text)\n",
    "    return text.strip(\"_\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# NORMALIZACIÃ“N DE COLUMNAS / HOJAS (ULTRA RÃPIDO)\n",
    "# ============================================================\n",
    "def normalize_sheet_name(name):\n",
    "    return normalize_fast(name)\n",
    "\n",
    "def normalize_column_name(name):\n",
    "    return normalize_fast(name)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# UNIFICADOR DE COLUMNAS (OPTIMIZADO)\n",
    "# ============================================================\n",
    "def unify_column_names_fast(dfs, cutoff=0.75):\n",
    "    \"\"\"\n",
    "    Mucho mÃ¡s rÃ¡pido que difflib.\n",
    "    - Agrupa por hash prefix â†’ fuzzy matching solo cuando necesario.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) recolectar todas las columnas normalizadas\n",
    "    col_master = set()\n",
    "    normalized_per_df = []\n",
    "\n",
    "    for df in dfs:\n",
    "        norm_cols = [normalize_column_name(c) for c in df.columns]\n",
    "        normalized_per_df.append(norm_cols)\n",
    "        col_master.update(norm_cols)\n",
    "\n",
    "    col_master = list(col_master)\n",
    "\n",
    "    # 2) bucketing por primeros 4 caracteres (reduce fuzzy 80%)\n",
    "    buckets = {}\n",
    "    for c in col_master:\n",
    "        key = c[:4]               # bucket rÃ¡pido, casi cero colisiones\n",
    "        buckets.setdefault(key, []).append(c)\n",
    "\n",
    "    # 3) renombrar cada df\n",
    "    results = []\n",
    "    for df, cols_norm in zip(dfs, normalized_per_df):\n",
    "        df_new = df.copy()\n",
    "        rename_map = {}\n",
    "\n",
    "        for orig, norm in zip(df.columns, cols_norm):\n",
    "            bucket = buckets.get(norm[:4], [])\n",
    "\n",
    "            # si hay match exacto â†’ directo\n",
    "            if norm in bucket:\n",
    "                rename_map[orig] = norm\n",
    "                continue\n",
    "\n",
    "            # fuzzy matching SOLO si hay pocos candidatos\n",
    "            best = None\n",
    "            best_score = cutoff\n",
    "            for cand in bucket:\n",
    "                score = fuzz.ratio(norm, cand) / 100\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best = cand\n",
    "\n",
    "            rename_map[orig] = best if best else norm\n",
    "\n",
    "        df_new = df_new.rename(columns=rename_map)\n",
    "        results.append(df_new)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# LECTURA Y CONCATENACIÃ“N POR HOJA (OPTIMIZADO)\n",
    "# ========\n",
    "\n",
    "# ============================================================\n",
    "# LECTURA Y CONCATENACIÃ“N POR HOJA (OPTIMIZADO + DEBUG)\n",
    "# ============================================================\n",
    "def leer_y_concatenar_por_hoja(lista_rutas, join=\"intersection\", fuzzy_cutoff=0.6, verbose=True):\n",
    "    grupos = {}  # nombre hoja normalizada â†’ lista dfs\n",
    "\n",
    "    # ======================================================\n",
    "    # 1) LECTURA DE ARCHIVOS Y AGRUPACIÃ“N POR HOJA\n",
    "    # ======================================================\n",
    "    for ruta in lista_rutas:\n",
    "        nombre_archivo = os.path.basename(ruta)\n",
    "        if verbose:\n",
    "            print(f\"\\nðŸ“„ Archivo detectado: {nombre_archivo}\")\n",
    "\n",
    "        mes = extraer_mes_nomb_excel(nombre_archivo)\n",
    "        aÃ±o = extraer_aÃ±o(nombre_archivo)\n",
    "\n",
    "        try:\n",
    "            xls = pd.ExcelFile(ruta)\n",
    "            if verbose:\n",
    "                print(f\"   â†’ Contiene hojas: {xls.sheet_names}\")\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"   âŒ Error al abrir: {e}\")\n",
    "            continue\n",
    "\n",
    "        for hoja in xls.sheet_names:\n",
    "            hoja_norm = normalize_sheet_name(hoja)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"   ðŸ“ Leyendo hoja '{hoja}' â†’ normalizada como '{hoja_norm}'\")\n",
    "\n",
    "            try:\n",
    "                df = xls.parse(hoja)\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"      âŒ Error al leer hoja '{hoja}': {e}\")\n",
    "                continue\n",
    "\n",
    "            df = df.copy()\n",
    "            df[\"mes\"] = mes\n",
    "            df[\"aÃ±o\"] = aÃ±o\n",
    "\n",
    "            # agrega df al grupo\n",
    "            grupos.setdefault(hoja_norm, []).append(df)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"      âœ” Agregada al grupo '{hoja_norm}'. TamaÃ±o actual: {len(grupos[hoja_norm])}\")\n",
    "\n",
    "    # ======================================================\n",
    "    # 2) PROCESAR CADA GRUPO\n",
    "    # ======================================================\n",
    "    resultados = {}\n",
    "\n",
    "    for hoja_norm, lista_df in grupos.items():\n",
    "        if verbose:\n",
    "            print(f\"\\nðŸ”· Procesando grupo '{hoja_norm}' con {len(lista_df)} hojas...\")\n",
    "\n",
    "        # 2.1 Normalizar columnas\n",
    "        if verbose:\n",
    "            print(f\"   ðŸ” Normalizando columnas (fuzzy cutoff={fuzzy_cutoff})â€¦\")\n",
    "\n",
    "        lista_df_ren = unify_column_names_fast(lista_df, cutoff=fuzzy_cutoff)\n",
    "\n",
    "        # Mostrar columnas normalizadas comparativamente\n",
    "        if verbose:\n",
    "            for i, (df_before, df_after) in enumerate(zip(lista_df, lista_df_ren), start=1):\n",
    "                print(f\"\\n   ðŸ“‘ Archivo {i}: columnas originales â†’ normalizadas:\")\n",
    "                print(f\"     Original : {list(df_before.columns)}\")\n",
    "                print(f\"     Normaliz.: {list(df_after.columns)}\")\n",
    "\n",
    "        # 2.2 Sacar universo de columnas\n",
    "        col_sets = [set(df.columns) for df in lista_df_ren]\n",
    "\n",
    "        if join == \"intersection\":\n",
    "            columnas_finales = sorted(set.intersection(*col_sets))\n",
    "            if verbose:\n",
    "                print(f\"\\n   ðŸ”— Usando INTERSECCIÃ“N de columnas:\")\n",
    "        else:\n",
    "            columnas_finales = sorted(set.union(*col_sets))\n",
    "            if verbose:\n",
    "                print(f\"\\n   ðŸ”— Usando UNIÃ“N de columnas:\")\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"      â†’ Columnas finales: {columnas_finales}\")\n",
    "\n",
    "        # 2.3 Alinear DataFrames\n",
    "        dfs_alineados = [df.reindex(columns=columnas_finales) for df in lista_df_ren]\n",
    "\n",
    "        # 2.4 Concatenar\n",
    "        if verbose:\n",
    "            print(f\"\\n   ðŸ“š Concatenando {len(dfs_alineados)} DataFramesâ€¦\")\n",
    "\n",
    "        df_final = pd.concat(dfs_alineados, ignore_index=True)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"   âœ” Grupo '{hoja_norm}' concatenado. Filas totales: {len(df_final)}\")\n",
    "\n",
    "        resultados[hoja_norm] = df_final\n",
    "\n",
    "    return resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96d72d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "from rapidfuzz import fuzz, process as fuzzprocess\n",
    "\n",
    "# ============================================================\n",
    "# PRECOMPILAR REGEX (20x mÃ¡s rÃ¡pido)\n",
    "# ============================================================\n",
    "_re_non_alnum = re.compile(r\"[^0-9a-z]+\")\n",
    "_re_multi_unders = re.compile(r\"_+\")\n",
    "\n",
    "# ============================================================\n",
    "# NORMALIZACIÃ“N RÃPIDA\n",
    "# ============================================================\n",
    "def normalize_fast(text: str) -> str:\n",
    "    \"\"\"Normaliza texto MUY rÃ¡pido: sin acentos, minÃºsculas, underscores limpios.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    text = \"\".join(c for c in text if not unicodedata.combining(c))\n",
    "    text = text.lower()\n",
    "\n",
    "    text = _re_non_alnum.sub(\"_\", text)\n",
    "    text = _re_multi_unders.sub(\"_\", text)\n",
    "    return text.strip(\"_\")\n",
    "\n",
    "\n",
    "def normalize_sheet_name(name):\n",
    "    \"\"\"Normaliza y singulariza para evitar 'vehiculos' / 'vehiculo'.\"\"\"\n",
    "    norm = normalize_fast(name)\n",
    "    # singularizaciÃ³n simple\n",
    "    if norm.endswith(\"es\") and len(norm) > 3:\n",
    "        norm = norm[:-2]\n",
    "    elif norm.endswith(\"s\") and len(norm) > 2:\n",
    "        norm = norm[:-1]\n",
    "    return norm\n",
    "\n",
    "\n",
    "def normalize_column_name(name):\n",
    "    return normalize_fast(name)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# UNIFICADOR DE COLUMNAS (OPTIMIZADO)\n",
    "# ============================================================\n",
    "def unify_column_names_fast(dfs, cutoff=0.75):\n",
    "    \"\"\"\n",
    "    Unifica nombres de columnas con:\n",
    "    - normalizaciÃ³n\n",
    "    - bucketing por prefijo (reduce fuzzy match 80%)\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------- recolectar columnas normalizadas ---------------\n",
    "    col_master = set()\n",
    "    normalized_per_df = []\n",
    "\n",
    "    for df in dfs:\n",
    "        norm_cols = [normalize_column_name(c) for c in df.columns]\n",
    "        normalized_per_df.append(norm_cols)\n",
    "        col_master.update(norm_cols)\n",
    "\n",
    "    col_master = list(col_master)\n",
    "\n",
    "    # --------------- buckets por primeros 4 caracteres ---------------\n",
    "    buckets = {}\n",
    "    for col in col_master:\n",
    "        key = col[:4]\n",
    "        buckets.setdefault(key, []).append(col)\n",
    "\n",
    "    # --------------- renombrar DataFrames ---------------\n",
    "    results = []\n",
    "    for df, norm_cols in zip(dfs, normalized_per_df):\n",
    "        rename_map = {}\n",
    "        for orig, norm in zip(df.columns, norm_cols):\n",
    "            candidates = buckets.get(norm[:4], [])\n",
    "\n",
    "            if norm in candidates:\n",
    "                rename_map[orig] = norm\n",
    "                continue\n",
    "\n",
    "            # fuzzy solo cuando necesario\n",
    "            best, score = fuzzprocess.extractOne(norm, candidates, scorer=fuzz.ratio) or (None, 0)\n",
    "            rename_map[orig] = best if score >= cutoff * 100 else norm\n",
    "\n",
    "        results.append(df.rename(columns=rename_map))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# AGRUPADOR DE HOJAS SIMILARES (vehiculo / vehiculos / vehÃ­culo)\n",
    "# ============================================================\n",
    "def agrupar_nombres_hoja(sheet_names, cutoff=80):\n",
    "    \"\"\"\n",
    "    Une nombres similares en un solo grupo:\n",
    "    Ej: vehiculo, vehiculos, vehÃ­culo â†’ vehiculo\n",
    "    \"\"\"\n",
    "    grupos = {}\n",
    "    canonicos = []\n",
    "\n",
    "    for name in sheet_names:\n",
    "        norm = normalize_sheet_name(name)\n",
    "\n",
    "        if canonicos:\n",
    "            match = fuzzprocess.extractOne(norm, canonicos, scorer=fuzz.ratio)\n",
    "            if match and match[1] >= cutoff:\n",
    "                grupos[match[0]].append(name)\n",
    "                continue\n",
    "\n",
    "        grupos[norm] = [name]\n",
    "        canonicos.append(norm)\n",
    "\n",
    "    return grupos\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# LECTURA GLOBAL\n",
    "# ============================================================\n",
    "def leer_y_concatenar_por_hoja(lista_rutas, join=\"intersection\", fuzzy_cutoff=0.6, verbose=True):\n",
    "    grupos = {}  # hoja_normalizada : list_dfs\n",
    "\n",
    "    # ======================================================\n",
    "    # 1) PRIMER PASO: Recolectar todas las hojas para agrupar\n",
    "    # ======================================================\n",
    "    if verbose:\n",
    "        print(\"\\nðŸ” Analizando nombres de hojas para detectar equivalencias...\")\n",
    "\n",
    "    all_sheet_names = []\n",
    "    file_sheets = {}\n",
    "\n",
    "    for ruta in lista_rutas:\n",
    "        try:\n",
    "            xls = pd.ExcelFile(ruta)\n",
    "            file_sheets[ruta] = xls.sheet_names\n",
    "            all_sheet_names.extend(xls.sheet_names)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"   ðŸ“„ {os.path.basename(ruta)} â†’ hojas: {xls.sheet_names}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            if verbose: print(f\"   âŒ Error leyendo {ruta}: {e}\")\n",
    "            continue\n",
    "\n",
    "    equivalencias = agrupar_nombres_hoja(all_sheet_names, cutoff=80)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nðŸ“Œ Grupos de nombres de hojas detectados:\")\n",
    "        for canon, items in equivalencias.items():\n",
    "            print(f\"   - {canon}: {items}\")\n",
    "\n",
    "    # ======================================================\n",
    "    # 2) LECTURA DE ARCHIVOS USANDO LOS GRUPOS DEFINIDOS\n",
    "    # ======================================================\n",
    "    for ruta in lista_rutas:\n",
    "        nombre_archivo = os.path.basename(ruta)\n",
    "        if verbose:\n",
    "            print(f\"\\nðŸ“‚ Procesando archivo: {nombre_archivo}\")\n",
    "\n",
    "        try:\n",
    "            xls = pd.ExcelFile(ruta)\n",
    "        except Exception as e:\n",
    "            if verbose: print(f\"   âŒ Error abriendo archivo: {e}\")\n",
    "            continue\n",
    "\n",
    "        for hoja_original in xls.sheet_names:\n",
    "            hoja_norm = normalize_sheet_name(hoja_original)\n",
    "\n",
    "            # buscar canonical del grupo\n",
    "            hoja_final = hoja_norm\n",
    "            for canon, miembros in equivalencias.items():\n",
    "                if hoja_original in miembros or hoja_norm == canon:\n",
    "                    hoja_final = canon\n",
    "                    break\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"   ðŸ“ Leyendo hoja '{hoja_original}' â†’ grupo '{hoja_final}'\")\n",
    "\n",
    "            try:\n",
    "                df = xls.parse(hoja_original).copy()\n",
    "            except Exception as e:\n",
    "                if verbose: print(f\"      âŒ Error al leer hoja: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # agregar mes y aÃ±o\n",
    "            mes = extraer_mes_nomb_excel(nombre_archivo)\n",
    "            aÃ±o = extraer_aÃ±o(nombre_archivo)\n",
    "            df[\"mes\"] = mes\n",
    "            df[\"aÃ±o\"] = aÃ±o\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"      âž• Columnas agregadas: mes={mes}, aÃ±o={aÃ±o}\")\n",
    "                print(f\"      ðŸ“ TamaÃ±o DF: {df.shape}\")\n",
    "\n",
    "            grupos.setdefault(hoja_final, []).append(df)\n",
    "\n",
    "    # ======================================================\n",
    "    # 3) PROCESAR CADA GRUPO\n",
    "    # ======================================================\n",
    "    resultados = {}\n",
    "\n",
    "    for hoja_norm, lista_df in grupos.items():\n",
    "        if verbose:\n",
    "            print(f\"\\nðŸ”· Procesando grupo '{hoja_norm}' ({len(lista_df)} DataFrames)â€¦\")\n",
    "\n",
    "        # 1) normalizar columnas\n",
    "        if verbose:\n",
    "            print(\"   ðŸ”§ Normalizando nombres de columnas...\")\n",
    "\n",
    "        lista_df_norm = unify_column_names_fast(lista_df, cutoff=fuzzy_cutoff)\n",
    "\n",
    "        # 2) inter/union de columnas\n",
    "        col_sets = [set(df.columns) for df in lista_df_norm]\n",
    "\n",
    "        columnas_finales = (\n",
    "            sorted(set.intersection(*col_sets))\n",
    "            if join == \"intersection\"\n",
    "            else sorted(set.union(*col_sets))\n",
    "        )\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"   ðŸ§© Columnas finales ({len(columnas_finales)}):\")\n",
    "            print(f\"      {columnas_finales}\")\n",
    "\n",
    "        # 3) alinear\n",
    "        if verbose:\n",
    "            print(\"   ðŸ“ Alineando columnas...\")\n",
    "\n",
    "        dfs_alineados = [df.reindex(columns=columnas_finales) for df in lista_df_norm]\n",
    "\n",
    "        # 4) concatenar\n",
    "        df_final = pd.concat(dfs_alineados, ignore_index=True)\n",
    "        resultados[hoja_norm] = df_final\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"   âœ” Concatenado completo: {len(df_final)} filas finales\")\n",
    "\n",
    "    return resultados\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d144543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Analizando nombres de hojas para detectar equivalencias...\n",
      "   ðŸ“„ Base de Datos de Transporte PÃºblico - Abril 2021.xlsx â†’ hojas: ['Servicios', 'Replegal', 'VehÃ­culos', 'O-D Recorridos', 'Trazados']\n",
      "   ðŸ“„ Base de Datos de Transporte PÃºblico - Agosto 2021.xlsx â†’ hojas: ['Servicios', 'RepLegales', 'VehÃ­culo', 'O-D Recorrido', 'Trazados']\n",
      "   ðŸ“„ Base de Datos de Transporte PÃºblico - Diciembre 2021.xlsx â†’ hojas: ['Servicios', 'RepLegales', 'VehÃ­culo', 'O-D Recorridos', 'Trazado']\n",
      "   ðŸ“„ Base de Datos de Transporte PÃºblico - Enero 2021.xlsx â†’ hojas: ['Servicios', 'RepLegales', 'Vehiculos', 'O-D Recorridos', 'Trazados']\n",
      "   ðŸ“„ Base de Datos de Transporte PÃºblico - Febrero 2021.xlsx â†’ hojas: ['Servicio', 'RepLegales', 'VehÃ­culo', 'O-D Recorrido', 'Trazado']\n",
      "   ðŸ“„ Base de Datos de Transporte PÃºblico - Julio 2021.xlsx â†’ hojas: ['Servicios', 'Representantes Legales', 'VehÃ­culos', 'O-D Recorridos', 'Trazados']\n",
      "   ðŸ“„ Base de Datos de Transporte PÃºblico - Junio 2021.xlsx â†’ hojas: ['Servicios', 'RepLegales', 'Vehiculos', 'O-D Recorridos', 'Trazados ']\n",
      "   ðŸ“„ Base de Datos de Transporte PÃºblico - Marzo 2021.xlsx â†’ hojas: ['Servicio', 'RepLegales', 'Vehiculos vig', 'O-D Recorrido', 'Trazados']\n",
      "   ðŸ“„ Base de Datos de Transporte PÃºblico - Mayo 2021.xlsx â†’ hojas: ['Servicios', 'RepLegales', 'Vehiculos', 'O-D Recorridos', 'Trazados']\n",
      "   ðŸ“„ Base de Datos de Transporte PÃºblico - Noviembre 2021.xlsx â†’ hojas: ['Servicio', 'RepLegales', 'VehÃ­culo', 'O-D Recorridos', 'Trazados']\n",
      "   ðŸ“„ Base de Datos de Transporte PÃºblico - Octubre 2021.xlsx â†’ hojas: ['Servicios', 'RepLegales', 'Vehiculos', 'O-D Recorridos', 'Trazados']\n",
      "   ðŸ“„ Base de Datos de Transporte PÃºblico - Septiembre 2021.xlsx â†’ hojas: ['Servicios', 'RepLegales', 'VehÃ­culos', 'O-D Recorridos', 'Trazados']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m bases \u001b[38;5;241m=\u001b[39m \u001b[43mleer_y_concatenar_por_hoja\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrutas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# ejemplo: imprimir tamaÃ±os de cada base\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m columnas, df \u001b[38;5;129;01min\u001b[39;00m bases\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[1;32mIn[6], line 135\u001b[0m, in \u001b[0;36mleer_y_concatenar_por_hoja\u001b[1;34m(lista_rutas, join, fuzzy_cutoff, verbose)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ruta \u001b[38;5;129;01min\u001b[39;00m lista_rutas:\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 135\u001b[0m         xls \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m         file_sheets[ruta] \u001b[38;5;241m=\u001b[39m xls\u001b[38;5;241m.\u001b[39msheet_names\n\u001b[0;32m    137\u001b[0m         all_sheet_names\u001b[38;5;241m.\u001b[39mextend(xls\u001b[38;5;241m.\u001b[39msheet_names)\n",
      "File \u001b[1;32mc:\\Users\\hsakurada\\AppData\\Local\\anaconda3\\envs\\base-2.1\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1580\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   1577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m=\u001b[39m engine\n\u001b[0;32m   1578\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options \u001b[38;5;241m=\u001b[39m storage_options\n\u001b[1;32m-> 1580\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engines\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1581\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_io\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1584\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hsakurada\\AppData\\Local\\anaconda3\\envs\\base-2.1\\Lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:553\u001b[0m, in \u001b[0;36mOpenpyxlReader.__init__\u001b[1;34m(self, filepath_or_buffer, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;124;03mReader using openpyxl engine.\u001b[39;00m\n\u001b[0;32m    543\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;124;03m    Arbitrary keyword arguments passed to excel engine.\u001b[39;00m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    552\u001b[0m import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenpyxl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 553\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hsakurada\\AppData\\Local\\anaconda3\\envs\\base-2.1\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:582\u001b[0m, in \u001b[0;36mBaseExcelReader.__init__\u001b[1;34m(self, filepath_or_buffer, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_workbook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    584\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\hsakurada\\AppData\\Local\\anaconda3\\envs\\base-2.1\\Lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:570\u001b[0m, in \u001b[0;36mOpenpyxlReader.load_workbook\u001b[1;34m(self, filepath_or_buffer, engine_kwargs)\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_workbook\u001b[39m(\n\u001b[0;32m    566\u001b[0m     \u001b[38;5;28mself\u001b[39m, filepath_or_buffer: FilePath \u001b[38;5;241m|\u001b[39m ReadBuffer[\u001b[38;5;28mbytes\u001b[39m], engine_kwargs\n\u001b[0;32m    567\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Workbook:\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenpyxl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_workbook\n\u001b[1;32m--> 570\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_workbook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hsakurada\\AppData\\Local\\anaconda3\\envs\\base-2.1\\Lib\\site-packages\\openpyxl\\reader\\excel.py:348\u001b[0m, in \u001b[0;36mload_workbook\u001b[1;34m(filename, read_only, keep_vba, data_only, keep_links, rich_text)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Open the given filename and return the workbook\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \n\u001b[0;32m    320\u001b[0m \u001b[38;5;124;03m:param filename: the path to open or a file-like object\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    344\u001b[0m \n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    346\u001b[0m reader \u001b[38;5;241m=\u001b[39m ExcelReader(filename, read_only, keep_vba,\n\u001b[0;32m    347\u001b[0m                      data_only, keep_links, rich_text)\n\u001b[1;32m--> 348\u001b[0m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reader\u001b[38;5;241m.\u001b[39mwb\n",
      "File \u001b[1;32mc:\\Users\\hsakurada\\AppData\\Local\\anaconda3\\envs\\base-2.1\\Lib\\site-packages\\openpyxl\\reader\\excel.py:291\u001b[0m, in \u001b[0;36mExcelReader.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_manifest()\n\u001b[0;32m    290\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread strings\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 291\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_strings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread workbook\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_workbook()\n",
      "File \u001b[1;32mc:\\Users\\hsakurada\\AppData\\Local\\anaconda3\\envs\\base-2.1\\Lib\\site-packages\\openpyxl\\reader\\excel.py:147\u001b[0m, in \u001b[0;36mExcelReader.read_strings\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    145\u001b[0m strings_path \u001b[38;5;241m=\u001b[39m ct\u001b[38;5;241m.\u001b[39mPartName[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marchive\u001b[38;5;241m.\u001b[39mopen(strings_path,) \u001b[38;5;28;01mas\u001b[39;00m src:\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshared_strings \u001b[38;5;241m=\u001b[39m \u001b[43mreader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hsakurada\\AppData\\Local\\anaconda3\\envs\\base-2.1\\Lib\\site-packages\\openpyxl\\reader\\strings.py:16\u001b[0m, in \u001b[0;36mread_string_table\u001b[1;34m(xml_source)\u001b[0m\n\u001b[0;32m     13\u001b[0m strings \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     14\u001b[0m STRING_TAG \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m}si\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m SHEET_MAIN_NS\n\u001b[1;32m---> 16\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxml_source\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtag\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSTRING_TAG\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mText\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hsakurada\\AppData\\Local\\anaconda3\\envs\\base-2.1\\Lib\\xml\\etree\\ElementTree.py:1238\u001b[0m, in \u001b[0;36miterparse.<locals>.iterator\u001b[1;34m(source)\u001b[0m\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m pullparser\u001b[38;5;241m.\u001b[39mread_events()\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;66;03m# load event buffer\u001b[39;00m\n\u001b[1;32m-> 1238\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hsakurada\\AppData\\Local\\anaconda3\\envs\\base-2.1\\Lib\\zipfile\\__init__.py:989\u001b[0m, in \u001b[0;36mZipExtFile.read\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof:\n\u001b[1;32m--> 989\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_readbuffer \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[1;32mc:\\Users\\hsakurada\\AppData\\Local\\anaconda3\\envs\\base-2.1\\Lib\\zipfile\\__init__.py:1065\u001b[0m, in \u001b[0;36mZipExtFile._read1\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_type \u001b[38;5;241m==\u001b[39m ZIP_DEFLATED:\n\u001b[0;32m   1064\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(n, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMIN_READ_SIZE)\n\u001b[1;32m-> 1065\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decompressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1066\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39meof \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1067\u001b[0m                  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_left \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m                  \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail)\n\u001b[0;32m   1069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bases = leer_y_concatenar_por_hoja(rutas)\n",
    "\n",
    "# ejemplo: imprimir tamaÃ±os de cada base\n",
    "for columnas, df in bases.items():\n",
    "    print(columnas, df.shape)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base-2.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
